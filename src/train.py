import pandas as pd
import joblib
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE  # Î•Î¹ÏƒÎ±Î³Ï‰Î³Î® SMOTE
from scipy.stats import randint

# Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½
df = pd.read_csv('data/diabetes.csv')

# Î§Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬ ÎºÎ±Î¹ ÎµÏ„Î¹ÎºÎ­Ï„ÎµÏ‚
X = df.drop('Outcome', axis=1)
y = df['Outcome']

# ÎšÎ±Î½Î¿Î½Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ· Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Î•Ï†Î±ÏÎ¼Î¿Î³Î® SMOTE Î³Î¹Î± oversampling
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_scaled, y)

# Î”Î¹Î±Ï‡Ï‰ÏÎ¹ÏƒÎ¼ÏŒÏ‚ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ (Î¼Îµ Ï„Î·Î½ ÎµÏ€Î±Î½Î±Î´ÎµÎ¹Î³Î¼Î±Ï„Î¿Î»Î·Ï€Ï„Î·Î¼Î­Î½Î· ÎµÎºÎ´Î¿Ï‡Î®)
X_train, X_test, y_train, y_test = train_test_split(X_train_resampled, y_train_resampled, test_size=0.2, random_state=42)

# ÎŸÏÎ¹ÏƒÎ¼ÏŒÏ‚ Ï…Ï€ÎµÏÏ€Î±ÏÎ±Î¼Î­Ï„ÏÏ‰Î½ Î³Î¹Î± GridSearchCV
param_grid_rf = {
    'n_estimators': [50, 100, 150],
    'max_depth': [5, 10, None],
    'min_samples_split': [2, 5],
    'class_weight': ['balanced', None]
}

param_grid_xgb = {
    'n_estimators': [50, 100, 150],
    'learning_rate': [0.01, 0.1, 0.3],
    'max_depth': [3, 5, 7, 10],
    'subsample': [0.7, 0.8, 0.9],
    'colsample_bytree': [0.7, 0.8, 0.9]
}

# Î›Î¯ÏƒÏ„Î± Î¼Î¿Î½Ï„Î­Î»Ï‰Î½
models = {
    "RandomForest": RandomForestClassifier(random_state=42),
    "SVC": SVC(kernel='rbf', probability=True, class_weight="balanced", random_state=42),
    "LogisticRegression": LogisticRegression(class_weight="balanced", random_state=42),
    "XGBoost": XGBClassifier(eval_metric='logloss', use_label_encoder=False),  # Î§Ï‰ÏÎ¯Ï‚ use_label_encoder=True
    "AdaBoost": AdaBoostClassifier(random_state=42),
    "NeuralNetwork": MLPClassifier(hidden_layer_sizes=(50,), max_iter=1000, random_state=42)
}

# Î’ÎµÎ»Ï„Î¹ÏƒÏ„Î¿Ï€Î¿Î¯Î·ÏƒÎ· Ï…Ï€ÎµÏÏ€Î±ÏÎ±Î¼Î­Ï„ÏÏ‰Î½ Î¼Îµ GridSearchCV
best_model = None
best_score = 0

for name, model in models.items():
    try:
        # Î§ÏÎ®ÏƒÎ· GridSearchCV Î³Î¹Î± RandomForest
        if name == "RandomForest":
            grid_search = GridSearchCV(model, param_grid_rf, cv=5, n_jobs=-1)
            grid_search.fit(X_train, y_train)
            best_model_rf = grid_search.best_estimator_
            model = best_model_rf
            print(f"\nÎšÎ±Î»ÏÏ„ÎµÏÎµÏ‚ Ï…Ï€ÎµÏÏ€Î±ÏÎ¬Î¼ÎµÏ„ÏÎ¿Î¹ Î³Î¹Î± Random Forest: {grid_search.best_params_}")
        
        # Î§ÏÎ®ÏƒÎ· GridSearchCV Î³Î¹Î± XGBoost
        elif name == "XGBoost":
            grid_search = GridSearchCV(model, param_grid_xgb, cv=5, n_jobs=-1)
            grid_search.fit(X_train, y_train)
            best_model_xgb = grid_search.best_estimator_
            model = best_model_xgb
            print(f"\nÎšÎ±Î»ÏÏ„ÎµÏÎµÏ‚ Ï…Ï€ÎµÏÏ€Î±ÏÎ¬Î¼ÎµÏ„ÏÎ¿Î¹ Î³Î¹Î± XGBoost: {grid_search.best_params_}")

        # Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ· Ï„Î¿Ï… Î¼Î¿Î½Ï„Î­Î»Î¿Ï…
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        acc = accuracy_score(y_test, y_pred)
        
        print(f"\nğŸ“Œ Model: {name}")
        print(f"Accuracy: {acc:.4f}")
        print("Classification Report:\n", classification_report(y_test, y_pred))
        print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

        if acc > best_score:
            best_score = acc
            best_model = model
    except Exception as e:
        print(f"Î£Ï†Î¬Î»Î¼Î± Î¼Îµ Ï„Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿ {name}: {e}")

# Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Ï„Î¿Ï… ÎºÎ±Î»ÏÏ„ÎµÏÎ¿Ï… Î¼Î¿Î½Ï„Î­Î»Î¿Ï…
joblib.dump(best_model, 'models/diabetes_model.pkl')
print("\nâœ… Î¤Î¿ ÎºÎ±Î»ÏÏ„ÎµÏÎ¿ Î¼Î¿Î½Ï„Î­Î»Î¿ Î±Ï€Î¿Î¸Î·ÎºÎµÏÏ„Î·ÎºÎµ!")
